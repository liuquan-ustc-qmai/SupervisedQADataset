
# CoreTech-LLM: A LLM focusing on key technologies and IPC Q&A

## ğŸ“– Introduction
 <!-- CoreTech-LLMæ˜¯åŸºäºQwen2-7Bæ¶æ„ï¼Œé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒæµç¨‹æ„å»ºçš„é¢†åŸŸä¸“ç”¨å¤§æ¨¡å‹ã€‚
æœ¬æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µä¼˜åŒ–æ–¹æ¡ˆï¼š
â€‹é˜¶æ®µ1:â€‹å¢é‡é¢„è®­ç»ƒé˜¶æ®µâ€‹â€‹ï¼šåœ¨ç²¾é€‰çš„é¢†åŸŸè¯­æ–™ï¼ˆæ¶µç›–å›½å®¶ä¸“åˆ©å±€IPCåŠå…¶å¯¹åº”æè¿°æ–‡ä»¶ã€ä¸“åˆ©æ–‡æœ¬ç­‰é«˜è´¨é‡æ•°æ®æºï¼‰ä¸ŠæŒç»­è®­ç»ƒï¼Œå¢å¼ºæ¨¡å‹çš„åŸºç¡€è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼›
â€‹â€‹é˜¶æ®µ2:â€‹è¯¾ç¨‹å­¦ä¹ å¾®è°ƒé˜¶æ®µâ€‹â€‹ï¼šé‡‡ç”¨æ¸è¿›å¼è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰ç­–ç•¥ï¼Œé€šè¿‡éš¾åº¦åˆ†çº§çš„ç›‘ç£å¾®è°ƒï¼Œæ˜¾è‘—æå‡æ¨¡å‹åœ¨æŠ€æœ¯é¢†åŸŸçš„å…³é”®æŠ€æœ¯è¯†åˆ«å’ŒIPCåˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ -->
**CoreTech-LLM**â€‹ is a domain-specific LLM built upon the Qwen2.5-7B(base) architecture through a multi-stage training pipeline. The model adopts a dual-phase optimization approach:

1. Continue Pre-training Phaseâ€‹â€‹: Continuous training on curated domain corpora (including high-quality data sources such as IPC classification documents from national patent offices with corresponding description files, and patent texts) to enhance the model's fundamental semantic understanding capabilities.

2. Supervised FineTuning Phaseâ€‹â€‹: Implementation of progressive Curriculum Learning strategy through difficulty-graded supervised fine-tuning, significantly improving the model's performance in technical domain tasks including key technology identification and IPC classification.



<!-- - Hugging Face Demo: doing

We provide a simple Gradio-based interactive web interface. After the service is started, it can be accessed through a browser, enter a question, and the model will return an answer. The command is as follows:
```shell
python gradio_demo.py --base_model path_to_llama_hf_dir --lora_model path_to_lora_dir
```

Parameter Description:

- `--base_model {base_model}`: directory to store LLaMA model weights and configuration files in HF format, or use the HF Model Hub model call name
- `--lora_model {lora_model}`: The directory where the LoRA file is located, and the name of the HF Model Hub model can also be used. If the lora weights have been merged into the pre-trained model, delete the --lora_model parameter
- `--tokenizer_path {tokenizer_path}`: Store the directory corresponding to the tokenizer. If this parameter is not provided, its default value is the same as --lora_model; if the --lora_model parameter is not provided, its default value is the same as --base_model
- `--use_cpu`: use only CPU for inference
- `--gpus {gpu_ids}`: Specifies the number of GPU devices used, the default is 0. If using multiple GPUs, separate them with commas, such as 0,1,2 -->




## ğŸš€ Training Pipeline

### Stage 1: Continue Pretraining
<!-- åŸºäºqwen2.5-7bæ¨¡å‹ï¼Œåœ¨ç²¾é€‰ä¸“åˆ©è¯­æ–™ï¼ˆå«IPCåˆ†ç±»æ–‡æœ¬ã€ä¸“åˆ©è¯´æ˜ä¹¦ç­‰ï¼‰ä¸Šè¿›è¡Œé¢†åŸŸé€‚åº”è®­ç»ƒå¹¶ä½¿ç”¨WuDaoé€šç”¨è¯­æ–™è¿›è¡Œé€šç”¨èƒ½åŠ›é¢„è®­ç»ƒå¾—åˆ°CoreTech-LLM-baseæ¨¡å‹ã€‚ -->
CoreTech-LLM-base is developed through domain-adaptive training on carefully selected patent corpora (including IPC-classified texts and patent specifications) based on the Qwen2.5-7B model, supplemented with WuDao general corpus for general capability pretraining.


```shell
cd /path/to/LLaMA-Factory

FORCE_TORCHRUN=1 CUDA_VISIBLE_DEVICES=0,1,2,3 \
nohup bash -c "setsid llamafactory-cli train /path/to/pretrain.yaml" \
> pretrain.log 2>&1 & \
disown -h %%
```


### Stage 2: Supervised FineTuning
<!-- ä»é«˜è´¨é‡æ–‡çŒ®ä¸­ä»¥åŠå›½å®¶ä¸“åˆ©æ–‡ä»¶ä¸­æå–çš„é¢†åŸŸ-å…³é”®æŠ€æœ¯ã€é¢†åŸŸ-IPCåˆ†ç±»é—®ç­”å¯¹ -->
Based on the CoreTech-LLM-base model, the CoreTech-LLM-instruct model is obtained by using Domain-specific key technologies and domain-IPC classification Q&A pairs extracted from high-quality literature and national patent documents.

```shell
cd /path/to/LLaMA-Factory

FORCE_TORCHRUN=1 CUDA_VISIBLE_DEVICES=0,1,2,3 \
nohup bash -c "setsid llamafactory-cli train \
/path/to/run_sft.yaml" \
> sft.log 2>&1 & \
disown -h %%
```



## ğŸ’¾ Install
#### Install the requirements

```markdown
git clone https://github.com/liuquan-ustc-qmai/CoreTech-LLM.git

cd CoreTech-LLM

pip install -r requirements.txt
```

<!-- ### Hardware Requirement (VRAM)


| Train Method  | Bits |   7B  |  13B  |  30B  |   70B  |  110B  |  8x7B |  8x22B |
|-------|------| ----- | ----- | ----- | ------ | ------ | ----- | ------ |
| Full   | AMP  | 120GB | 240GB | 600GB | 1200GB | 2000GB | 900GB | 2400GB |
| Full   | 16   |  60GB | 120GB | 300GB |  600GB |  900GB | 400GB | 1200GB |
| LoRA  | 16   |  16GB |  32GB |  64GB |  160GB |  240GB | 120GB |  320GB |
| QLoRA | 8    |  10GB |  20GB |  40GB |   80GB |  140GB |  60GB |  160GB |
| QLoRA | 4    |   6GB |  12GB |  24GB |   48GB |   72GB |  30GB |   96GB |
| QLoRA | 2    |   4GB |   8GB |  16GB |   24GB |   48GB |  18GB |   48GB | -->


## ğŸ”¥ Inference
After the training is complete, now we load the trained model for local inference.

```shell
CUDA_VISIBLE_DEVICES=0 llamafactory-cli chat path/to/run_vllm.yaml

```

<!-- Parameter Description:

- `--base_model {base_model}`: Directory to store LLaMA model weights and configuration files in HF format
- `--lora_model {lora_model}`: The directory where the LoRA file is decompressed, and the name of the HF Model Hub model can also be used. If you have incorporated LoRA weights into the pre-trained model, you can not provide this parameter
- `--tokenizer_path {tokenizer_path}`: Store the directory corresponding to the tokenizer. If this parameter is not provided, its default value is the same as --lora_model; if the --lora_model parameter is not provided, its default value is the same as --base_model
- `--with_prompt`: Whether to merge the input with the prompt template. Be sure to enable this option if loading an Alpaca model!
- `--interactive`: start interactively for multiple single rounds of question and answer
- `--data_file {file_name}`: Start in non-interactive mode, read the contents of file_name line by line for prediction
- `--predictions_file {file_name}`: In non-interactive mode, write the predicted results to file_name in json format
- `--use_cpu`: use only CPU for inference
- `--gpus {gpu_ids}`: Specifies the number of GPU devices used, the default is 0. If using multiple GPUs, separate them with commas, such as 0,1,2 -->


#### Inference Examples



## âš ï¸ LICENSE

<!--The license agreement for the project code is [The Apache License 2.0](/LICENSE), the code is free for commercial use, and the model weights and data can only be used for research purposes. Please attach MedicalGPT's link and license agreement in the product description.-->

## ğŸ˜‡ Citation

<!-- If you used MedicalGPT in your research, please cite as follows:

```latex
@misc{MedicalGPT,
   title={MedicalGPT: Training Medical GPT Model},
   author={Ming Xu},
   year={2023},
   howpublished={\url{https://github.com/shibing624/MedicalGPT}},
}
```
-->

<!-- ## ğŸ’• Acknowledgements

- [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)
- [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)

Thanks for their great work!
#### Related Projects
- [shibing624/ChatPilot](https://github.com/shibing624/ChatPilot): Provide a simple and easy-to-use web UI interface for LLM Agent (including RAG, online search, code interpreter). -->
